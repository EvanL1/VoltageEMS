# Load Test Results Analyzer
FROM python:3.11-slim

RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Install Python dependencies
RUN pip install --no-cache-dir \
    pandas>=1.5.0 \
    matplotlib>=3.6.0 \
    seaborn>=0.12.0 \
    jinja2>=3.1.0 \
    pyyaml>=6.0

# Create load test analyzer script
RUN cat > /app/analyze_load_results.py << 'EOF'
#!/usr/bin/env python3
"""
Load Test Results Analyzer for VoltageEMS
Analyzes and reports on load test results from multiple tools
"""

import json
import os
import glob
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LoadTestAnalyzer:
    def __init__(self):
        self.results_dir = "/app/results"
        self.reports_dir = "/app/reports"
        self.analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'summary': {},
            'k6_results': {},
            'artillery_results': {},
            'jmeter_results': {},
            'performance_trends': {},
            'recommendations': []
        }

    def analyze_k6_results(self):
        """Analyze K6 test results"""
        logger.info("Analyzing K6 results...")
        
        k6_files = glob.glob(os.path.join(self.results_dir, "*k6*.json"))
        k6_analysis = {
            'total_tests': len(k6_files),
            'test_summaries': [],
            'aggregated_metrics': {}
        }
        
        all_metrics = {
            'http_req_duration': [],
            'http_req_failed': [],
            'http_reqs': [],
            'vus': []
        }
        
        for k6_file in k6_files:
            try:
                with open(k6_file, 'r') as f:
                    # K6 outputs NDJSON format
                    lines = f.readlines()
                    metrics_data = []
                    
                    for line in lines:
                        if line.strip():
                            data = json.loads(line)
                            if data.get('type') == 'Point':
                                metrics_data.append(data)
                    
                    if metrics_data:
                        test_summary = self.process_k6_metrics(metrics_data)
                        test_summary['filename'] = os.path.basename(k6_file)
                        k6_analysis['test_summaries'].append(test_summary)
                        
                        # Aggregate metrics
                        for metric, values in test_summary.get('metrics', {}).items():
                            if metric in all_metrics:
                                all_metrics[metric].extend(values if isinstance(values, list) else [values])
                        
            except Exception as e:
                logger.error(f"Error processing K6 file {k6_file}: {e}")
        
        # Calculate aggregated statistics
        for metric, values in all_metrics.items():
            if values:
                k6_analysis['aggregated_metrics'][metric] = {
                    'count': len(values),
                    'mean': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values),
                    'p95': sorted(values)[int(len(values) * 0.95)] if len(values) > 20 else max(values),
                    'p99': sorted(values)[int(len(values) * 0.99)] if len(values) > 100 else max(values)
                }
        
        self.analysis_results['k6_results'] = k6_analysis

    def process_k6_metrics(self, metrics_data):
        """Process K6 metrics data"""
        summary = {
            'total_requests': 0,
            'failed_requests': 0,
            'avg_response_time': 0,
            'max_response_time': 0,
            'metrics': {}
        }
        
        response_times = []
        failed_count = 0
        total_count = 0
        
        for point in metrics_data:
            metric = point.get('metric', '')
            value = point.get('value', 0)
            
            if metric == 'http_req_duration':
                response_times.append(value)
            elif metric == 'http_req_failed':
                if value > 0:
                    failed_count += 1
                total_count += 1
            elif metric == 'http_reqs':
                summary['total_requests'] += 1
        
        if response_times:
            summary['avg_response_time'] = sum(response_times) / len(response_times)
            summary['max_response_time'] = max(response_times)
            summary['metrics']['response_times'] = response_times
        
        if total_count > 0:
            summary['failed_requests'] = failed_count
            summary['error_rate'] = failed_count / total_count
        
        return summary

    def analyze_artillery_results(self):
        """Analyze Artillery test results"""
        logger.info("Analyzing Artillery results...")
        
        artillery_files = glob.glob(os.path.join(self.results_dir, "*artillery*.json"))
        artillery_analysis = {
            'total_tests': len(artillery_files),
            'test_summaries': []
        }
        
        for artillery_file in artillery_files:
            try:
                with open(artillery_file, 'r') as f:
                    data = json.load(f)
                
                summary = {
                    'filename': os.path.basename(artillery_file),
                    'aggregate': data.get('aggregate', {}),
                    'intermediate': data.get('intermediate', [])
                }
                
                # Process aggregate stats
                agg = data.get('aggregate', {})
                if agg:
                    summary['total_requests'] = agg.get('counters', {}).get('http.requests', 0)
                    summary['total_errors'] = agg.get('counters', {}).get('errors.total', 0)
                    summary['avg_response_time'] = agg.get('latency', {}).get('mean', 0)
                    summary['p95_response_time'] = agg.get('latency', {}).get('p95', 0)
                    summary['p99_response_time'] = agg.get('latency', {}).get('p99', 0)
                
                artillery_analysis['test_summaries'].append(summary)
                
            except Exception as e:
                logger.error(f"Error processing Artillery file {artillery_file}: {e}")
        
        self.analysis_results['artillery_results'] = artillery_analysis

    def analyze_jmeter_results(self):
        """Analyze JMeter test results"""
        logger.info("Analyzing JMeter results...")
        
        jmeter_files = glob.glob(os.path.join(self.results_dir, "*jmeter*.jtl"))
        jmeter_analysis = {
            'total_tests': len(jmeter_files),
            'test_summaries': []
        }
        
        for jmeter_file in jmeter_files:
            try:
                # JMeter JTL files are CSV format
                df = pd.read_csv(jmeter_file)
                
                summary = {
                    'filename': os.path.basename(jmeter_file),
                    'total_samples': len(df),
                    'successful_samples': len(df[df['success'] == True]),
                    'failed_samples': len(df[df['success'] == False]),
                    'error_rate': len(df[df['success'] == False]) / len(df) * 100,
                    'avg_response_time': df['elapsed'].mean(),
                    'median_response_time': df['elapsed'].median(),
                    'p90_response_time': df['elapsed'].quantile(0.90),
                    'p95_response_time': df['elapsed'].quantile(0.95),
                    'p99_response_time': df['elapsed'].quantile(0.99),
                    'max_response_time': df['elapsed'].max(),
                    'throughput': len(df) / (df['timeStamp'].max() - df['timeStamp'].min()) * 1000  # requests per second
                }
                
                jmeter_analysis['test_summaries'].append(summary)
                
            except Exception as e:
                logger.error(f"Error processing JMeter file {jmeter_file}: {e}")
        
        self.analysis_results['jmeter_results'] = jmeter_analysis

    def generate_performance_insights(self):
        """Generate performance insights and recommendations"""
        logger.info("Generating performance insights...")
        
        insights = []
        recommendations = []
        
        # Analyze K6 results for insights
        k6_results = self.analysis_results.get('k6_results', {})
        if k6_results.get('aggregated_metrics'):
            metrics = k6_results['aggregated_metrics']
            
            # Response time analysis
            if 'http_req_duration' in metrics:
                duration_stats = metrics['http_req_duration']
                avg_duration = duration_stats['mean']
                p95_duration = duration_stats['p95']
                
                if avg_duration > 500:  # 500ms threshold
                    insights.append(f"Average response time ({avg_duration:.0f}ms) exceeds 500ms threshold")
                    recommendations.append("Consider optimizing database queries and Redis operations")
                
                if p95_duration > 1000:  # 1s threshold for P95
                    insights.append(f"95th percentile response time ({p95_duration:.0f}ms) exceeds 1s")
                    recommendations.append("Investigate slow queries and consider caching improvements")
            
            # Error rate analysis
            if 'http_req_failed' in metrics:
                failed_stats = metrics['http_req_failed']
                error_rate = failed_stats['mean'] * 100
                
                if error_rate > 5:  # 5% error rate threshold
                    insights.append(f"Error rate ({error_rate:.1f}%) exceeds 5% threshold")
                    recommendations.append("Review error logs and improve error handling")
        
        # Analyze Artillery results
        artillery_results = self.analysis_results.get('artillery_results', {})
        for test in artillery_results.get('test_summaries', []):
            if test.get('total_errors', 0) / max(test.get('total_requests', 1), 1) > 0.1:
                insights.append(f"High error rate in Artillery test: {test['filename']}")
                recommendations.append("Check service stability under sustained load")
        
        # Analyze JMeter results
        jmeter_results = self.analysis_results.get('jmeter_results', {})
        for test in jmeter_results.get('test_summaries', []):
            if test.get('error_rate', 0) > 10:
                insights.append(f"High error rate ({test['error_rate']:.1f}%) in JMeter test")
            
            if test.get('p95_response_time', 0) > 2000:
                insights.append(f"High P95 response time ({test['p95_response_time']:.0f}ms) in JMeter test")
                recommendations.append("Consider horizontal scaling or performance optimization")
        
        # General recommendations
        if not recommendations:
            recommendations.append("Performance metrics are within acceptable thresholds")
            recommendations.append("Consider establishing performance baseline for regression testing")
        
        self.analysis_results['performance_insights'] = insights
        self.analysis_results['recommendations'] = recommendations

    def generate_html_report(self):
        """Generate comprehensive HTML report"""
        logger.info("Generating HTML report...")
        
        html_template = """
<!DOCTYPE html>
<html>
<head>
    <title>VoltageEMS Load Test Analysis Report</title>
    <style>
        body { font-family: 'Segoe UI', Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .header { text-align: center; border-bottom: 3px solid #2c3e50; padding-bottom: 20px; margin-bottom: 30px; }
        .header h1 { color: #2c3e50; margin: 0; }
        .summary { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 30px 0; }
        .metric-card { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 8px; text-align: center; }
        .metric-card h3 { margin: 0 0 10px 0; }
        .metric-card .value { font-size: 2em; font-weight: bold; }
        .section { margin: 30px 0; }
        .section h2 { color: #2c3e50; border-bottom: 2px solid #ecf0f1; padding-bottom: 10px; }
        .insights { background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px; padding: 20px; margin: 20px 0; }
        .recommendations { background: #d4edda; border: 1px solid #badbcc; border-radius: 8px; padding: 20px; margin: 20px 0; }
        .test-results { overflow-x: auto; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background-color: #f8f9fa; font-weight: 600; }
        .status-pass { color: #28a745; font-weight: bold; }
        .status-warn { color: #ffc107; font-weight: bold; }
        .status-fail { color: #dc3545; font-weight: bold; }
        .charts { display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); gap: 20px; margin: 20px 0; }
        .chart-placeholder { background: #f8f9fa; border: 2px dashed #dee2e6; height: 300px; display: flex; align-items: center; justify-content: center; color: #6c757d; border-radius: 8px; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>VoltageEMS Load Test Analysis Report</h1>
            <p>Generated: {{ timestamp }}</p>
        </div>
        
        <div class="summary">
            <div class="metric-card">
                <h3>Total Tests</h3>
                <div class="value">{{ total_tests }}</div>
            </div>
            <div class="metric-card">
                <h3>Avg Response Time</h3>
                <div class="value">{{ avg_response_time }}ms</div>
            </div>
            <div class="metric-card">
                <h3>Error Rate</h3>
                <div class="value">{{ error_rate }}%</div>
            </div>
            <div class="metric-card">
                <h3>Total Requests</h3>
                <div class="value">{{ total_requests }}</div>
            </div>
        </div>
        
        {% if insights %}
        <div class="section">
            <h2>Performance Insights</h2>
            <div class="insights">
                <h3>Key Findings</h3>
                <ul>
                {% for insight in insights %}
                    <li>{{ insight }}</li>
                {% endfor %}
                </ul>
            </div>
        </div>
        {% endif %}
        
        {% if recommendations %}
        <div class="section">
            <h2>Recommendations</h2>
            <div class="recommendations">
                <h3>Action Items</h3>
                <ul>
                {% for recommendation in recommendations %}
                    <li>{{ recommendation }}</li>
                {% endfor %}
                </ul>
            </div>
        </div>
        {% endif %}
        
        {% if k6_results.test_summaries %}
        <div class="section">
            <h2>K6 Test Results</h2>
            <div class="test-results">
                <table>
                    <tr>
                        <th>Test</th>
                        <th>Total Requests</th>
                        <th>Failed Requests</th>
                        <th>Error Rate</th>
                        <th>Avg Response Time</th>
                        <th>Max Response Time</th>
                    </tr>
                    {% for test in k6_results.test_summaries %}
                    <tr>
                        <td>{{ test.filename }}</td>
                        <td>{{ test.total_requests }}</td>
                        <td>{{ test.failed_requests }}</td>
                        <td class="{% if test.error_rate > 0.05 %}status-fail{% elif test.error_rate > 0.01 %}status-warn{% else %}status-pass{% endif %}">
                            {{ (test.error_rate * 100)|round(2) }}%
                        </td>
                        <td>{{ test.avg_response_time|round(0) }}ms</td>
                        <td>{{ test.max_response_time|round(0) }}ms</td>
                    </tr>
                    {% endfor %}
                </table>
            </div>
        </div>
        {% endif %}
        
        {% if artillery_results.test_summaries %}
        <div class="section">
            <h2>Artillery Test Results</h2>
            <div class="test-results">
                <table>
                    <tr>
                        <th>Test</th>
                        <th>Total Requests</th>
                        <th>Total Errors</th>
                        <th>Avg Response Time</th>
                        <th>P95 Response Time</th>
                        <th>P99 Response Time</th>
                    </tr>
                    {% for test in artillery_results.test_summaries %}
                    <tr>
                        <td>{{ test.filename }}</td>
                        <td>{{ test.total_requests }}</td>
                        <td>{{ test.total_errors }}</td>
                        <td>{{ test.avg_response_time|round(0) }}ms</td>
                        <td>{{ test.p95_response_time|round(0) }}ms</td>
                        <td>{{ test.p99_response_time|round(0) }}ms</td>
                    </tr>
                    {% endfor %}
                </table>
            </div>
        </div>
        {% endif %}
        
        {% if jmeter_results.test_summaries %}
        <div class="section">
            <h2>JMeter Test Results</h2>
            <div class="test-results">
                <table>
                    <tr>
                        <th>Test</th>
                        <th>Total Samples</th>
                        <th>Error Rate</th>
                        <th>Avg Response Time</th>
                        <th>P95 Response Time</th>
                        <th>Throughput (req/s)</th>
                    </tr>
                    {% for test in jmeter_results.test_summaries %}
                    <tr>
                        <td>{{ test.filename }}</td>
                        <td>{{ test.total_samples }}</td>
                        <td class="{% if test.error_rate > 10 %}status-fail{% elif test.error_rate > 5 %}status-warn{% else %}status-pass{% endif %}">
                            {{ test.error_rate|round(2) }}%
                        </td>
                        <td>{{ test.avg_response_time|round(0) }}ms</td>
                        <td>{{ test.p95_response_time|round(0) }}ms</td>
                        <td>{{ test.throughput|round(1) }}</td>
                    </tr>
                    {% endfor %}
                </table>
            </div>
        </div>
        {% endif %}
        
        <div class="section">
            <h2>Performance Charts</h2>
            <div class="charts">
                <div class="chart-placeholder">
                    Response Time Distribution<br>
                    <small>(Charts would be generated with actual data)</small>
                </div>
                <div class="chart-placeholder">
                    Throughput Over Time<br>
                    <small>(Charts would be generated with actual data)</small>
                </div>
            </div>
        </div>
        
        <div class="section">
            <h2>Test Environment</h2>
            <p><strong>Test Duration:</strong> Various (see individual test results)</p>
            <p><strong>Load Pattern:</strong> Mixed workload with ramp-up phases</p>
            <p><strong>Target Services:</strong> All VoltageEMS microservices</p>
            <p><strong>Infrastructure:</strong> Docker containers with resource limits</p>
        </div>
    </div>
</body>
</html>
        """
        
        try:
            from jinja2 import Template
            template = Template(html_template)
            
            # Calculate summary metrics
            total_tests = (self.analysis_results['k6_results'].get('total_tests', 0) + 
                          self.analysis_results['artillery_results'].get('total_tests', 0) + 
                          self.analysis_results['jmeter_results'].get('total_tests', 0))
            
            # Get average response time from K6 results
            k6_metrics = self.analysis_results.get('k6_results', {}).get('aggregated_metrics', {})
            avg_response_time = k6_metrics.get('http_req_duration', {}).get('mean', 0)
            
            # Calculate error rate
            error_rate = 0
            if 'http_req_failed' in k6_metrics:
                error_rate = k6_metrics['http_req_failed'].get('mean', 0) * 100
            
            # Estimate total requests
            total_requests = k6_metrics.get('http_reqs', {}).get('count', 0)
            
            html_content = template.render(
                timestamp=self.analysis_results['timestamp'],
                total_tests=total_tests,
                avg_response_time=f"{avg_response_time:.0f}",
                error_rate=f"{error_rate:.1f}",
                total_requests=total_requests,
                insights=self.analysis_results.get('performance_insights', []),
                recommendations=self.analysis_results.get('recommendations', []),
                k6_results=self.analysis_results['k6_results'],
                artillery_results=self.analysis_results['artillery_results'],
                jmeter_results=self.analysis_results['jmeter_results']
            )
            
            os.makedirs(self.reports_dir, exist_ok=True)
            with open(os.path.join(self.reports_dir, 'load_test_report.html'), 'w') as f:
                f.write(html_content)
                
            logger.info("HTML report generated successfully")
            
        except Exception as e:
            logger.error(f"Error generating HTML report: {e}")

    def save_json_results(self):
        """Save analysis results as JSON"""
        os.makedirs(self.reports_dir, exist_ok=True)
        with open(os.path.join(self.reports_dir, 'load_test_analysis.json'), 'w') as f:
            json.dump(self.analysis_results, f, indent=2)

    def run(self):
        """Run the complete analysis"""
        logger.info("Starting load test analysis...")
        
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # Analyze results from different tools
        self.analyze_k6_results()
        self.analyze_artillery_results()
        self.analyze_jmeter_results()
        
        # Generate insights and recommendations
        self.generate_performance_insights()
        
        # Generate reports
        self.generate_html_report()
        self.save_json_results()
        
        logger.info("Load test analysis completed successfully")

if __name__ == "__main__":
    analyzer = LoadTestAnalyzer()
    analyzer.run()
EOF

RUN chmod +x /app/analyze_load_results.py

CMD ["python", "/app/analyze_load_results.py"]