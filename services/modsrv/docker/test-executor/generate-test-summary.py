#!/usr/bin/env python3
"""ç”Ÿæˆæµ‹è¯•ç»“æœæ‘˜è¦æŠ¥å‘Š"""

import json
from pathlib import Path
from datetime import datetime


class TestSummaryGenerator:
    """æµ‹è¯•æ‘˜è¦ç”Ÿæˆå™¨"""

    def __init__(self, results_dir: str = "test-results"):
        self.results_dir = Path(results_dir)
        self.summary_data = {
            "generation_time": datetime.now().isoformat(),
            "results_directory": str(self.results_dir),
            "api_tests": {},
            "template_tests": {},
            "integration_tests": {},
            "performance_data": {},
            "total_statistics": {},
        }

    def collect_api_test_results(self):
        """æ”¶é›†APIæµ‹è¯•ç»“æœ"""
        api_messages_dir = self.results_dir / "api-messages"

        if not api_messages_dir.exists():
            return

        api_stats = {
            "health_check": {"count": 0, "avg_response_time": 0},
            "model_list": {"count": 0, "avg_response_time": 0},
            "model_detail": {"count": 0, "avg_response_time": 0},
            "control_commands": {"count": 0, "avg_response_time": 0},
            "performance": {"count": 0, "avg_response_time": 0},
        }

        for category in api_stats.keys():
            category_dir = api_messages_dir / category
            if category_dir.exists():
                message_files = list(category_dir.glob("*.json"))
                api_stats[category]["count"] = len(message_files)

                # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
                response_times = []
                for msg_file in message_files:
                    try:
                        with open(msg_file, "r", encoding="utf-8") as f:
                            msg_data = json.load(f)

                        if (
                            "metadata" in msg_data
                            and "response_time_ms" in msg_data["metadata"]
                        ):
                            response_times.append(
                                msg_data["metadata"]["response_time_ms"]
                            )
                    except:
                        continue

                if response_times:
                    api_stats[category]["avg_response_time"] = round(
                        sum(response_times) / len(response_times), 2
                    )

        self.summary_data["api_tests"] = api_stats

    def collect_template_test_results(self):
        """æ”¶é›†æ¨¡æ¿æµ‹è¯•ç»“æœ"""
        template_tests_dir = self.results_dir / "template-tests"

        if not template_tests_dir.exists():
            return

        template_stats = {
            "templates_discovered": 0,
            "models_generated": 0,
            "categories_tested": [],
            "success_rate": 0,
        }

        # è¯»å–æµ‹è¯•ç»“æœæ–‡ä»¶
        results_file = template_tests_dir / "template_test_results.json"
        if results_file.exists():
            try:
                with open(results_file, "r", encoding="utf-8") as f:
                    results_data = json.load(f)

                template_stats["models_generated"] = len(
                    results_data.get("generated_models", [])
                )

                # ç»Ÿè®¡æµ‹è¯•ç±»åˆ«
                categories = set()
                for model in results_data.get("generated_models", []):
                    if "category" in model:
                        categories.add(model["category"])

                template_stats["categories_tested"] = list(categories)

                # è®¡ç®—æˆåŠŸç‡
                subtests = [
                    "template_discovery",
                    "template_loading",
                    "variable_extraction",
                    "model_building",
                ]
                passed = sum(1 for test in subtests if results_data.get(test, False))
                template_stats["success_rate"] = round(
                    (passed / len(subtests)) * 100, 1
                )

            except Exception as e:
                print(f"è¯»å–æ¨¡æ¿æµ‹è¯•ç»“æœå¤±è´¥: {e}")

        # ç»Ÿè®¡ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶
        model_files = list(template_tests_dir.glob("built_*.yml"))
        template_stats["templates_discovered"] = len(model_files)

        self.summary_data["template_tests"] = template_stats

    def collect_integration_test_results(self):
        """æ”¶é›†é›†æˆæµ‹è¯•ç»“æœ"""
        # æŸ¥æ‰¾ä¸»è¦çš„æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶
        test_report_files = list(self.results_dir.glob("test-report*.json"))
        api_report_files = list(self.results_dir.glob("api_test_report*.json"))

        integration_stats = {
            "total_test_runs": len(test_report_files),
            "api_test_runs": len(api_report_files),
            "last_test_results": {},
            "overall_success_rate": 0,
        }

        # åˆ†ææœ€æ–°çš„æµ‹è¯•æŠ¥å‘Š
        if test_report_files:
            latest_report = max(test_report_files, key=lambda f: f.stat().st_mtime)
            try:
                with open(latest_report, "r", encoding="utf-8") as f:
                    report_data = json.load(f)

                if "test_execution" in report_data:
                    test_exec = report_data["test_execution"]
                    integration_stats["last_test_results"] = {
                        "timestamp": test_exec.get("timestamp", ""),
                        "total_tests": test_exec.get("total_tests", 0),
                        "passed_tests": test_exec.get("passed_tests", 0),
                        "failed_tests": test_exec.get("failed_tests", 0),
                        "success_rate": test_exec.get("success_rate", 0),
                    }
                    integration_stats["overall_success_rate"] = test_exec.get(
                        "success_rate", 0
                    )
            except Exception as e:
                print(f"è¯»å–é›†æˆæµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")

        self.summary_data["integration_tests"] = integration_stats

    def collect_performance_data(self):
        """æ”¶é›†æ€§èƒ½æ•°æ®"""
        performance_dir = self.results_dir / "api-messages" / "performance"

        perf_stats = {
            "performance_test_runs": 0,
            "latest_performance": {},
            "response_time_trends": {},
        }

        if performance_dir.exists():
            perf_files = list(performance_dir.glob("performance_test_*.json"))
            perf_stats["performance_test_runs"] = len(perf_files)

            # åˆ†ææœ€æ–°çš„æ€§èƒ½æµ‹è¯•
            if perf_files:
                latest_perf = max(perf_files, key=lambda f: f.stat().st_mtime)
                try:
                    with open(latest_perf, "r", encoding="utf-8") as f:
                        perf_data = json.load(f)

                    if "statistics" in perf_data:
                        perf_stats["latest_performance"] = perf_data["statistics"]
                except Exception as e:
                    print(f"è¯»å–æ€§èƒ½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")

        self.summary_data["performance_data"] = perf_stats

    def calculate_total_statistics(self):
        """è®¡ç®—æ€»ä½“ç»Ÿè®¡"""
        total_stats = {
            "total_api_messages": 0,
            "total_models_generated": 0,
            "total_test_files": 0,
            "avg_api_response_time": 0,
            "overall_health_score": 0,
        }

        # APIæ¶ˆæ¯ç»Ÿè®¡
        api_count = 0
        total_response_time = 0
        response_count = 0

        for category, stats in self.summary_data["api_tests"].items():
            api_count += stats.get("count", 0)
            if stats.get("avg_response_time", 0) > 0:
                total_response_time += stats["avg_response_time"]
                response_count += 1

        total_stats["total_api_messages"] = api_count
        if response_count > 0:
            total_stats["avg_api_response_time"] = round(
                total_response_time / response_count, 2
            )

        # æ¨¡æ¿ç”Ÿæˆç»Ÿè®¡
        total_stats["total_models_generated"] = self.summary_data["template_tests"].get(
            "models_generated", 0
        )

        # æµ‹è¯•æ–‡ä»¶ç»Ÿè®¡
        if self.results_dir.exists():
            all_files = list(self.results_dir.rglob("*"))
            total_stats["total_test_files"] = len([f for f in all_files if f.is_file()])

        # æ•´ä½“å¥åº·åº¦è¯„åˆ† (0-100)
        health_factors = []

        # APIæµ‹è¯•å¥åº·åº¦
        if api_count > 0:
            health_factors.append(min(100, api_count * 10))  # æ¯ä¸ªAPIæ¶ˆæ¯+10åˆ†ï¼Œæœ€é«˜100

        # å“åº”æ—¶é—´å¥åº·åº¦
        avg_response = total_stats["avg_api_response_time"]
        if avg_response > 0:
            if avg_response < 100:  # <100ms = ä¼˜ç§€
                health_factors.append(100)
            elif avg_response < 500:  # <500ms = è‰¯å¥½
                health_factors.append(80)
            elif avg_response < 1000:  # <1s = ä¸€èˆ¬
                health_factors.append(60)
            else:  # >1s = è¾ƒå·®
                health_factors.append(40)

        # æ¨¡æ¿ç³»ç»Ÿå¥åº·åº¦
        template_success = self.summary_data["template_tests"].get("success_rate", 0)
        if template_success > 0:
            health_factors.append(template_success)

        # é›†æˆæµ‹è¯•å¥åº·åº¦
        integration_success = self.summary_data["integration_tests"].get(
            "overall_success_rate", 0
        )
        if integration_success > 0:
            health_factors.append(integration_success)

        if health_factors:
            total_stats["overall_health_score"] = round(
                sum(health_factors) / len(health_factors), 1
            )

        self.summary_data["total_statistics"] = total_stats

    def generate_summary_report(self, output_file: str = None):
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        print("ğŸ“Š æ”¶é›†æµ‹è¯•ç»“æœæ•°æ®...")

        self.collect_api_test_results()
        self.collect_template_test_results()
        self.collect_integration_test_results()
        self.collect_performance_data()
        self.calculate_total_statistics()

        if not output_file:
            output_file = (
                self.results_dir
                / f"test_summary_{int(datetime.now().timestamp())}.json"
            )
        else:
            output_file = Path(output_file)

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_file.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜JSONæ ¼å¼
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.summary_data, f, ensure_ascii=False, indent=2)

        # ç”Ÿæˆå¯è¯»æ ¼å¼çš„æ‘˜è¦
        readable_file = output_file.with_suffix(".md")
        self.generate_readable_summary(readable_file)

        print("âœ… æµ‹è¯•æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"  ğŸ“„ JSONæ ¼å¼: {output_file}")
        print(f"  ğŸ“– å¯è¯»æ ¼å¼: {readable_file}")

        return output_file

    def generate_readable_summary(self, output_file: Path):
        """ç”Ÿæˆå¯è¯»çš„æ‘˜è¦æŠ¥å‘Š"""
        content = f"""# ModSrv æµ‹è¯•æ‘˜è¦æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {self.summary_data["generation_time"]}
ç»“æœç›®å½•: {self.summary_data["results_directory"]}

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

- **APIæ¶ˆæ¯æ€»æ•°**: {self.summary_data["total_statistics"].get("total_api_messages", 0)}
- **ç”Ÿæˆæ¨¡å‹æ•°**: {self.summary_data["total_statistics"].get("total_models_generated", 0)}
- **æµ‹è¯•æ–‡ä»¶æ•°**: {self.summary_data["total_statistics"].get("total_test_files", 0)}
- **å¹³å‡å“åº”æ—¶é—´**: {self.summary_data["total_statistics"].get("avg_api_response_time", 0)}ms
- **æ•´ä½“å¥åº·åº¦**: {self.summary_data["total_statistics"].get("overall_health_score", 0)}/100

## ğŸ”Œ APIæµ‹è¯•ç»“æœ

"""

        for category, stats in self.summary_data["api_tests"].items():
            content += f"### {category}\n"
            content += f"- æ¶ˆæ¯æ•°: {stats.get('count', 0)}\n"
            content += f"- å¹³å‡å“åº”æ—¶é—´: {stats.get('avg_response_time', 0)}ms\n\n"

        template_stats = self.summary_data["template_tests"]
        content += f"""## ğŸ”§ æ¨¡æ¿ç³»ç»Ÿæµ‹è¯•

- **å‘ç°æ¨¡æ¿æ•°**: {template_stats.get("templates_discovered", 0)}
- **ç”Ÿæˆæ¨¡å‹æ•°**: {template_stats.get("models_generated", 0)}
- **æµ‹è¯•ç±»åˆ«**: {", ".join(template_stats.get("categories_tested", []))}
- **æˆåŠŸç‡**: {template_stats.get("success_rate", 0)}%

"""

        integration_stats = self.summary_data["integration_tests"]
        last_test = integration_stats.get("last_test_results", {})
        content += f"""## ğŸ§ª é›†æˆæµ‹è¯•ç»“æœ

- **æµ‹è¯•è¿è¡Œæ¬¡æ•°**: {integration_stats.get("total_test_runs", 0)}
- **APIæµ‹è¯•è¿è¡Œæ¬¡æ•°**: {integration_stats.get("api_test_runs", 0)}
- **æ•´ä½“æˆåŠŸç‡**: {integration_stats.get("overall_success_rate", 0)}%

### æœ€è¿‘æµ‹è¯•ç»“æœ
- æµ‹è¯•æ—¶é—´: {last_test.get("timestamp", "N/A")}
- æ€»æµ‹è¯•æ•°: {last_test.get("total_tests", 0)}
- é€šè¿‡æµ‹è¯•: {last_test.get("passed_tests", 0)}
- å¤±è´¥æµ‹è¯•: {last_test.get("failed_tests", 0)}
- æˆåŠŸç‡: {last_test.get("success_rate", 0)}%

"""

        perf_stats = self.summary_data["performance_data"]
        latest_perf = perf_stats.get("latest_performance", {})
        content += f"""## âš¡ æ€§èƒ½æµ‹è¯•æ•°æ®

- **æ€§èƒ½æµ‹è¯•è¿è¡Œæ¬¡æ•°**: {perf_stats.get("performance_test_runs", 0)}

"""

        if latest_perf:
            content += "### æœ€æ–°æ€§èƒ½æ•°æ®\n"
            for test_type, metrics in latest_perf.items():
                if isinstance(metrics, dict):
                    content += f"#### {test_type}\n"
                    content += (
                        f"- å¹³å‡å“åº”æ—¶é—´: {metrics.get('avg_response_time', 0)}ms\n"
                    )
                    content += (
                        f"- æœ€å¤§å“åº”æ—¶é—´: {metrics.get('max_response_time', 0)}ms\n"
                    )
                    content += (
                        f"- æœ€å°å“åº”æ—¶é—´: {metrics.get('min_response_time', 0)}ms\n"
                    )
                    content += f"- æˆåŠŸç‡: {metrics.get('success_rate', 0)}%\n\n"

        content += """
---
*æŠ¥å‘Šç”±ModSrvæµ‹è¯•ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""

        with open(output_file, "w", encoding="utf-8") as f:
            f.write(content)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ç”ŸæˆModSrvæµ‹è¯•æ‘˜è¦æŠ¥å‘Š")
    parser.add_argument(
        "--results-dir", default="test-results", help="æµ‹è¯•ç»“æœç›®å½•è·¯å¾„"
    )
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    try:
        generator = TestSummaryGenerator(args.results_dir)
        output_file = generator.generate_summary_report(args.output)

        print(f"\nğŸ‰ æµ‹è¯•æ‘˜è¦ç”Ÿæˆå®Œæˆ: {output_file}")

    except Exception as e:
        print(f"âŒ ç”Ÿæˆæµ‹è¯•æ‘˜è¦å¤±è´¥: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
